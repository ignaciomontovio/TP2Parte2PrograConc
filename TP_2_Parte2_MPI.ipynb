{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ignaciomontovio/TP2Parte2PrograConc/blob/beta/TP_2_Parte2_MPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Now3GWleQC3h"
      },
      "source": [
        "# Programación Concurrente - TP2 - Parte 2\n",
        "\n",
        "Para este ejercicio se ha optado por aplicar el tema teórico **MPI** (Message Passing Interface). La finalidad del ejercicio es ampliar el conocimiento sobras la manera que posee Python para implementar la comunicación entre distintos procesos con el uso de una librería denominada **MPI4py** [1]. Para más información puede consultar la última revisión en [2]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqlIsawOQ4Ly"
      },
      "source": [
        "---\n",
        "## 2.1. Ejercicio - Hola Mundo con MPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxoup5gcQxvi"
      },
      "source": [
        "### 2.1.1. Preguntas del ejercicio\n",
        "\n",
        "Ejecute este ejemplo para $4$ o más instancias y responda:\n",
        "\n",
        "a) ¿Qué función realiza la instancia maestra? ¿Qué función realizan las instancias esclavas?\n",
        "\n",
        "b) ¿Cuántas de esas instancias ejecuta la función `main()`, `initWork()` y `doWork()`?\n",
        "\n",
        "c) ¿Cómo se diferencian los mensajes de trabajo o de finalización?\n",
        "\n",
        "d) ¿Cómo implementaría la función BLAS `axpy()` con este patrón?¿Sería eficiente? *Tips*: Pide solo el planteo, no la implementación.\n",
        "\n",
        "e) ¿Qué sucede cuando solo ejecuta con una sola instancia?\n",
        "\n",
        "f) *Punto opcional*: El código que ejecutan las instancias esclavas, tienen un error en su lógica. ¿Cómo se podría solucionar?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Respuestas\n",
        "\n",
        "a) La funcion que realiza la instancia maestra (PROCESO MAESTRO) es controlar, coordinar y supervisar el trabajo que llevan a cabo los demas procesos que se implementan (PROCESO ESCLAVO). Ademas se encarga de la asignacion de trabajo, el envio de tareas a los procesos esclavos y de recibir las respuestas que estos le devuelven. Por ultimo se ocupa de marcar la tarjeta y finalizar el trabajo cuando se completan todas las tareas.\n",
        "Por otro lado la instancia esclava esta a la espera de recibir tareas proporcionadas por el proceso maestro, apenas llegan realiza su trabajo correspondiente y envian los resultados al proceso maestro. Una instancia esclava se da por finalizada cuando recibe el mensaje \"END_WORK_FLAG\" marcando ademas su tarjeta.\n",
        "\n",
        "b) main(): Se ejecutara una vez por el proceso maestro mas x cantidad de veces, una por cada proceso esclavo creado.\n",
        "\n",
        "initWork(): Solo se ejecutara una vez por el proceso maestro.\n",
        "\n",
        "doWork(): Se ejecutara multiples veces, es decir, una vez por cada proceso esclavo.\n",
        "\n",
        "c) Los mensajes de trabajo y de finalizacion se diferencian a traves de la utilizacion de etiquetas entre las comunicaciones del proceso mestro y el proceso esclavo. Utilizando WORK_FLAG para enviar tareas de trabajo a los procesos esclavos y END_WORK_FLAG para enviar el mensaje de finalizacion a los procesos esclavos.\n",
        "\n",
        "d) La funcion BLAS axpy() se utiliza para realizar la siguiente operacion \"Y=aX+Y\", donde: \n",
        "    Y: es un vector (array) al que se le suma una combinación lineal de otro vector X.\n",
        "    X: es otro vector.\n",
        "    a: es un escalar que multiplica el vector X antes de sumarse a Y.\n",
        "\n",
        "La implementacion propuesta seria:\n",
        "\n",
        "1- El proceso maestro sera el responsable de dividir la operacion axpy() en partes mas pequeñas, es decir, dividir los vectores y distribuir estas partes a los procesos esclavos.\n",
        "\n",
        "2- Cada proceso esclavo recibira esta parte fragmentada de los datos y realizara la operacion axpy(). Al finalizar enviaran el resultado hacia el proceso maestro.\n",
        "\n",
        "3- El proceso maestro recibira los reusltados de cada uno de los procesos esclavos y los combinara para obener el resultado final de la operacion axpy()\n",
        "\n",
        "La eficiencia dependera de la distribucion del trabajo, el tamaño de los vectores y la comunicacion entre los procesos. Lo que si ocurriria seria que dividiriamos el trabajo total por la cantidad de procesos esclavos que tangamos, aprovechando asi el paralelismo.\n",
        "\n",
        "e) Si se ejecuta una sola instancia no se aprovechara el paralelismo de MPI en la ejecucion. El codigo se ejecutara de manera secuencial en un solo proceso, pudiendose ver el mismo con la instancia maestra.\n",
        "\n",
        "f) El error del codigo radica en el orden de ejecucion de **comm.recv()** y **stat.Get_tag()**. Debido a que se esta pardiendo el estado final en la ultima operacion de recepcion, ya que no se esta almacenando.\n",
        "\n",
        "### Codigo modificado\n",
        "\n",
        "    def doWork(comm):\n",
        "        while(True):\n",
        "            stat = MPI.Status() # Obtiene el estado actual del empleado.\n",
        "            comm.recv(source=0, tag=MPI.ANY_TAG, status=stat) # Obtiene lo enviado por el Jefe.\n",
        "            waitTime =stat.Get_tag()\n",
        "\n",
        "            if (waitTime == END_WORK_FLAG):\n",
        "                print(\"Marca tarjeta el empleado {}.\".format(comm.Get_rank()), flush=True)\n",
        "                return\n",
        "            else:\n",
        "                print(\"Soy el empleado con id {}, toca descanzo por {} hs.\".format(comm.Get_rank(), waitTime), flush=True)\n",
        "\n",
        "            time.sleep(waitTime)\n",
        "            comm.send(waitTime, dest=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKxaao0vQ9zn"
      },
      "source": [
        "###2.1.2. Armado del ambiente\n",
        "\n",
        "Es en donde se instalar, por única vez, el módulo MPI4py de Python en el cuaderno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0cOYbcMbOTy",
        "outputId": "5d262775-e6e8-47ff-effb-a3cdbe5a13bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mpi4py\n",
            "  Obtaining dependency information for mpi4py from https://files.pythonhosted.org/packages/d6/5e/94811843d8c99729bf2b73341ef74b5874c661b8110dec690fe315dddd7e/mpi4py-3.1.5-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading mpi4py-3.1.5-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
            "Downloading mpi4py-3.1.5-cp311-cp311-win_amd64.whl (465 kB)\n",
            "   ---------------------------------------- 0.0/466.0 kB ? eta -:--:--\n",
            "    --------------------------------------- 10.2/466.0 kB ? eta -:--:--\n",
            "    --------------------------------------- 10.2/466.0 kB ? eta -:--:--\n",
            "   ----- --------------------------------- 71.7/466.0 kB 653.6 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 466.0/466.0 kB 4.2 MB/s eta 0:00:00\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: C:\\Users\\agust\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glBG6GGeRCFI"
      },
      "source": [
        "### 2.1.3. Código del programa en Lenguaje Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmIsWmLlbUYT",
        "outputId": "2fa2d39e-8ab2-4022-d8dd-f0af06ba387c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Ejercicio2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Ejercicio2.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# --------------------------------------------\n",
        "# Formulario\n",
        "Max_tiempo_sleep =   1#@param {type: \"slider\", min: 1, max: 10}\n",
        "Min_tiempo_sleep =   0#@param {type: \"slider\", min: 0, max: 10}\n",
        "# --------------------------------------------\n",
        "\n",
        "# --------------------------------------------\n",
        "# Constantes de comunicacion\n",
        "WORK_FLAG = 1\n",
        "END_WORK_FLAG = 2\n",
        "# --------------------------------------------\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD # Instanciamos el tipo de comunicador a utilizar.\n",
        "    id = comm.Get_rank()  # Obtenemos el id como atributo del proceso que se ejecuta.\n",
        "\n",
        "    # Utilizamos el 0 para definir al procesos Maestro, cualquier otro id sera un esclavo.\n",
        "    if (id == 0) :\n",
        "        init() # Llamamos funcion init para eventos que requeriremos inicialmente solo 1 vez.\n",
        "        numProcesses = comm.Get_size()  # Obtenemos el numero de procesos totales ejecutados.\n",
        "        numTasks = (numProcesses-1)*4 # Se setea el numero de tareas.\n",
        "        workTimes = generateTasks(numTasks) # Se generan las tareas, en este caso seran\n",
        "        print(\"El jefe crea {} horas de descanso de sus empleados:\".format(workTimes.size), flush=True)\n",
        "        print(workTimes, flush=True)\n",
        "        initWork(comm, workTimes, numProcesses)\n",
        "    else:\n",
        "        doWork(comm)\n",
        "\n",
        "def generateTasks(numTasks):\n",
        "    #TODO: Cambiar la semilla del random para que se generen efectivamente diferentes numeros.\n",
        "    np.random.seed(1000)\n",
        "    return np.random.randint(low=Min_tiempo_sleep, high=Max_tiempo_sleep, size=numTasks)\n",
        "\n",
        "def init():\n",
        "  print()\n",
        "  print(\"Version MPI4py utilizada: {}\".format(MPI.Get_version()), flush=True)\n",
        "  print()\n",
        "  print( \"------------------------------------\", flush=True)\n",
        "  print( \"Sistema de trabajo Suizo:\", flush=True)\n",
        "  print( \"------------------------------------\", flush=True)\n",
        "  print()\n",
        "\n",
        "def initWork(comm, workTimes, numProcesses):\n",
        "    totalWork = workTimes.size\n",
        "    workcount = 0\n",
        "    recvcount = 0\n",
        "\n",
        "    print(\"Jefe enviando las tareas iniciales:\", flush=True)\n",
        "    for id in range(1, numProcesses):\n",
        "        if workcount < totalWork:\n",
        "            work=workTimes[workcount]\n",
        "            comm.send(work, dest=id, tag=WORK_FLAG) # Envia mensaje de iniciar trabajo con el dato correspondiente del array.\n",
        "            workcount += 1\n",
        "            print(\"Jefe envia trabajo y {} hs de descanso al empleado {}.\".format(work, id), flush=True)\n",
        "    print( \"------------------------------------\", flush=True)\n",
        "\n",
        "    # Mientras haya trabajo, se recibe el resultado de los empleados y se sigue enviando MAS trabajo.\n",
        "    while (workcount < totalWork) :\n",
        "        stat = MPI.Status()\n",
        "        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat) # Recivimos resultados de los empleados.\n",
        "        recvcount += 1\n",
        "        workerId = stat.Get_source() # Obtenemos el identificador del empleado.\n",
        "        print(\"Jefe recibe trabajo completado {} del empleado {}.\".format(workTime, workerId), flush=True)\n",
        "        #send next work\n",
        "        work=workTimes[workcount]\n",
        "        comm.send(work, dest=workerId, tag=WORK_FLAG)\n",
        "        workcount += 1\n",
        "        print(\"Jefe envia nuevo trabajo y {} hs de descanso al empleado {}.\".format(work, workerId), flush=True)\n",
        "\n",
        "    while (recvcount < totalWork):\n",
        "        stat = MPI.Status()\n",
        "        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat)\n",
        "        recvcount += 1\n",
        "        workerId = stat.Get_source()\n",
        "        print(\"Jefe recibe trabajo completado {} del empleado {}.\".format(workTime, workerId), flush=True)\n",
        "\n",
        "    for id in range(1, numProcesses):\n",
        "        comm.send(0, dest=id, tag=END_WORK_FLAG)\n",
        "\n",
        "\n",
        "def doWork(comm):\n",
        "    while(True):\n",
        "        stat = MPI.Status() # Obtiene el estado actual del empleado.\n",
        "        waitTime = comm.recv(source=0, tag=MPI.ANY_TAG, status=stat) # Obtiene lo enviado por el Jefe.\n",
        "        print(\"Soy el empleado con id {}, toca descanzo por {} hs.\".format(comm.Get_rank(), waitTime), flush=True)\n",
        "\n",
        "        if (stat.Get_tag() == END_WORK_FLAG):\n",
        "            print(\"Marca tarjeta el empleado {}.\".format(comm.Get_rank()), flush=True)\n",
        "            return\n",
        "        time.sleep(waitTime)\n",
        "        comm.send(waitTime, dest=0)\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLQTe1yvRJuB"
      },
      "source": [
        "### 2.1.4 Ejecución del programa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "6G5pnJxBRMpl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'mpirun' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------\n",
        "#@title Parámetros de ejecución { vertical-output: true }\n",
        "NRO =   6#@param {type: \"number\"}\n",
        "# --------------------------------------------\n",
        "\n",
        "! mpirun --oversubscribe --allow-run-as-root -np $NRO python Ejercicio2.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o2cAPHNSOFD"
      },
      "source": [
        "---\n",
        "## 3.1 Ejercicio Contar palabras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM-wCiQ9aGtp"
      },
      "source": [
        "Desarrollar un programa que permita obtener el valor máximo de un conjunto dado de forma distribuida.\n",
        "\n",
        "Las condiciones a tener en cuenta son:\n",
        "\n",
        "\n",
        "*   Debe trabajar con al menos, 4 procesos.\n",
        "*   El resultado final debe ser informado en cada proceso.\n",
        "*   Implementar comunicación por Buffer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjEQLeUAT7VJ"
      },
      "source": [
        "###3.1.1 Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CANFX1w8lFwa",
        "outputId": "e1cc495e-7bee-46f5-cc38-db7a64fbf943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting mpi_tp4.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mpi_tp4.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size()\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "count = 5000\n",
        "sendbuf = None\n",
        "\n",
        "if rank == 0:\n",
        "  sendbuf = np.random.randn(size, count)\n",
        "  display(\"Máximo de matriz: \", sendbuf.max())\n",
        "\n",
        "###### Inicio de código ######\n",
        "recvbuf = np.empty(count, dtype=np.float64)\n",
        "comm.Scatter(sendbuf, recvbuf, root=0)\n",
        "\n",
        "max = recvbuf.max()\n",
        "recvbuf = np.empty(size, dtype=np.float64)\n",
        "comm.Gather(max, recvbuf, root=0)\n",
        "\n",
        "max_max = np.full(1, recvbuf.max())\n",
        "comm.Bcast(max_max, root=0)\n",
        "\n",
        "display(rank, \" - Máximo de matriz: \", max_max[0])\n",
        "##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOg_oyVlwHhJ",
        "outputId": "8138b27d-b48c-4ce6-bfe9-01a9f8593c0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Máximo de matriz:  4.291234630478944\n",
            "0  - Máximo de matriz:  4.291234630478944\n",
            "1  - Máximo de matriz:  4.291234630478944\n",
            "2  - Máximo de matriz:  4.291234630478944\n",
            "6  - Máximo de matriz:  4.291234630478944\n",
            "4  - Máximo de matriz:  4.291234630478944\n",
            "8  - Máximo de matriz:  4.291234630478944\n",
            "3  - Máximo de matriz:  4.291234630478944\n",
            "7  - Máximo de matriz:  4.291234630478944\n",
            "5  - Máximo de matriz:  4.291234630478944\n",
            "9  - Máximo de matriz:  4.291234630478944\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------\n",
        "# Formulario\n",
        "NRO =   10#@param {type: \"slider\", min: 4, max: 10}\n",
        "# --------------------------------------------\n",
        "\n",
        "! mpirun --oversubscribe --allow-run-as-root -np $NRO python mpi_tp4.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erCUx9dGQhrI"
      },
      "source": [
        "---\n",
        "# Bibliografía\n",
        "\n",
        "[1] MPI4py: https://mpi4py.readthedocs.io/en/stable/\n",
        "\n",
        "[2] La versión oficial de MPI (Versión 4): https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
